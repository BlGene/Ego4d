{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6aefec6a",
   "metadata": {},
   "source": [
    "# Ego4D Annotation Visualizations\n",
    "This notebook has visualizations for Ego4D's different annotation types\n",
    "\n",
    "## Prerequisites\n",
    "1. Use the [Ego4D CLI](https://ego4d-data.org/docs/start-here/) to download the annotations and full_scale datasets. This notebook expects all videos to be downloaded, but you can manually download them as you go.\n",
    "2. Install all the packages in this notebook using `requirements.txt`.\n",
    "\n",
    "## **Useful Links:**\n",
    "\n",
    "[Ego4D Docs - Start Here!](https://ego4d-data.org/docs/start-here/#Download-The-CLI)\n",
    "\n",
    "[Data Overview](https://ego4d-data.org/docs/data-overview/)\n",
    "\n",
    "[Official Ego4D Site](https://ego4d-data.org/)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79002be2",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55cfa2f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import os\n",
    "# Set your options here\n",
    "# Sampled Videos will be downloaded to <version>/full_scale/ if they aren't already there\n",
    "\n",
    "CLI_OUTPUT_DIR = \"/misc/lmbraid19/argusm/CLUSTER/ego4d_data\" # Replace with the full path to the --output_directory you pass to the cli\n",
    "VERSION = \"v2\"\n",
    "MANIFEST_PATH = os.path.join(CLI_OUTPUT_DIR, 'manifest.csv') # Use this if manifest is at <version>/manifest.csv\n",
    "# MANIFEST_PATH = os.path.join(CLI_OUTPUT_DIR, VERSION, 'full_scale' ,'manifest.csv') # Use this if manifest is at <version>/full_scale/\n",
    "\n",
    "assert os.path.exists(MANIFEST_PATH), f\"Manifest doesn't exist at {MANIFEST_PATH}. Is the CLI_OUTPUT_DIR right? Do you satisfy the pre-requisites?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8eb2908-4798-4d56-8dfa-29ab7f6d5f99",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70c50d69",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup\n",
    "import sys\n",
    "if os.path.abspath(\".\") not in sys.path: # Allow us to use util files in the same dir\n",
    "    sys.path.insert(0, os.path.abspath(\".\"))\n",
    "\n",
    "import av\n",
    "import collections\n",
    "import csv\n",
    "import cv2\n",
    "import functools\n",
    "import json\n",
    "import logging\n",
    "import math\n",
    "import matplotlib.collections as mc\n",
    "import matplotlib.image as mpimg\n",
    "import matplotlib.patches as mpatches\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.colors as mcolors\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "import uuid\n",
    "import warnings\n",
    "\n",
    "from celluloid import Camera\n",
    "from IPython.display import HTML\n",
    "from iopath.common.file_io import PathManager\n",
    "from itertools import groupby\n",
    "from pprint import pprint\n",
    "from nb_video_utils import _get_frames, _get_frames2\n",
    "\n",
    "%matplotlib inline\n",
    "plt.rcParams[\"animation.html\"] = \"jshtml\"\n",
    "\n",
    "pathmgr = PathManager()\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "def vid_df_des(df):\n",
    "    return f\"#{len(df)} {df.duration_sec.sum()/60/60:.1f}h\"\n",
    "def vid_des(videos):\n",
    "    return f\"#{len(videos)} {sum((x.duration_sec for x in videos))/60/60:.1f}h\"\n",
    "def deserialize_str_list(list_: str):\n",
    "    list_ = list_[1:-2]\n",
    "    items = list_.split(\"', '\")\n",
    "    return list(map(lambda z: z.strip(\"'\"), items))\n",
    "def to_1D(series):\n",
    "    return pd.Series([x for _list in series for x in _list])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f5e727d",
   "metadata": {},
   "source": [
    "# Video API\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76dd3ddb",
   "metadata": {},
   "outputs": [],
   "source": [
    "videos_df = pd.read_csv(MANIFEST_PATH)\n",
    "videos_df['scenarios'] = videos_df['scenarios'].apply(deserialize_str_list)\n",
    "def get_video(video_uid='353ae622-c322-443e-95b4-e9927dedfa1c'):\n",
    "    return videos_df.loc[videos_df['video_uid'] == video_uid].iloc[0]\n",
    "print(f\"R1 Videos: {vid_df_des(videos_df)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab6c6b9d",
   "metadata": {},
   "source": [
    "# Visualization Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5956880f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# in: video_path, frame_number, boxes: [{ object_type, bbox: {x, y, width, height} }]}, draw_labels\n",
    "# out: path to image of bboxes rendered onto the video frame\n",
    "\n",
    "def get_frames_from_video(video_path, frame_meta, format=\"rgb24\"):\n",
    "    frame_number_list = [f['frame_number'] for f in frame_meta]\n",
    "    frame_bboxes_list = [f['boxes'] for f in frame_meta]\n",
    "    with av.open(str(video_path)) as input_video:\n",
    "        for frame in _get_frames2(frame_number_list, input_video, include_audio=False, audio_buffer_frames=0):\n",
    "            yield frame.to_ndarray(format=format)\n",
    "\n",
    "def render_frames_with_bboxes(video_path, frame_meta, draw_bboxes=True, draw_labels=True, draw_arrows={}):\n",
    "    colormap = { # Custom colors for FHO annotations\n",
    "        'object_of_change': (0, 255, 255),\n",
    "        'left_hand': (0, 0, 255),\n",
    "        'right_hand': (0, 255, 0)\n",
    "    }\n",
    "    defaultColor = (255, 255, 0)\n",
    "    rect_thickness = 1\n",
    "    rectLineType = cv2.LINE_4\n",
    "    fontColor = (0, 0, 0)\n",
    "    fontFace = cv2.FONT_HERSHEY_DUPLEX\n",
    "    fontScale = 1\n",
    "    fontThickness = 1\n",
    "    \n",
    "    frame_number_list = [f['frame_number'] for f in frame_meta]\n",
    "    frame_bboxes_list = [f['boxes'] for f in frame_meta]\n",
    "    with av.open(str(video_path)) as input_video:\n",
    "        frames = list(_get_frames2(frame_number_list, input_video, include_audio=False, audio_buffer_frames=0))\n",
    "        paths = []\n",
    "        for frame, frame_number, boxes in zip(frames, frame_number_list, frame_bboxes_list):\n",
    "            img = frame.to_ndarray(format=\"bgr24\")\n",
    "            if draw_bboxes:\n",
    "                for box in boxes:\n",
    "                    label, bbox = box['object_type'], box['bbox']\n",
    "                    rectColor = colormap.get(label, defaultColor) if label else defaultColor\n",
    "                    x, y, width, height = list(map(lambda x: int(x), [bbox['x'], bbox['y'], bbox['width'], bbox['height']]))\n",
    "                    cv2.rectangle(img, pt1=(x,y), pt2=(x+width, y+height), color=rectColor, thickness=rect_thickness, lineType=rectLineType)\n",
    "                    if label and draw_labels:\n",
    "                        textSize, baseline = cv2.getTextSize(label, fontFace, fontScale, fontThickness)\n",
    "                        textWidth, textHeight = textSize\n",
    "                        cv2.rectangle(img, pt1=(x - rect_thickness//2, y - rect_thickness//2), pt2=(x + textWidth + 10 + rect_thickness, y - textHeight - 10 - rect_thickness), color=rectColor, thickness=-1)\n",
    "                        cv2.putText(img, text=label, org=(x + 10, y - 10), fontFace=fontFace, fontScale=fontScale, color=fontColor, thickness=fontThickness, lineType=cv2.LINE_AA)\n",
    "            for label, arrow_dict in draw_arrows.items():\n",
    "                arrColor = colormap.get(label, defaultColor) if label else defaultColor\n",
    "                cv2.arrowedLine(img, arrow_dict[\"start\"], arrow_dict[\"end\"],\n",
    "                                arrColor, rect_thickness, rectLineType)\n",
    "            path = f\"/tmp/{frame_number}_{str(uuid.uuid1())}.jpg\"\n",
    "            cv2.imwrite(path, img)\n",
    "            paths.append(path)\n",
    "    return paths\n",
    "\n",
    "# in: video_path, frames: [{ frame_number, frame_type, boxes: [{ object_type, bbox: {x, y, width, height} }] }]\n",
    "# out: void; as a side-effect, renders frames from the video with matplotlib\n",
    "def plot_frames_with_bboxes(video_path, frames, max_cols = 3):\n",
    "    cols = min(max_cols, len(frames))\n",
    "    rows = math.ceil(len(frames) / cols)\n",
    "    fig, axes = plt.subplots(nrows=rows, ncols=cols, figsize=(10*cols, 7 * rows))\n",
    "    if len(frames) > 1:\n",
    "        [axi.set_axis_off() for axi in axes.ravel()] # Hide axes\n",
    "    else:\n",
    "        axes = np.array([axes,])\n",
    "\n",
    "    frame_paths = render_frames_with_bboxes(video_path, frames)\n",
    "    for idx, (frame_data, frame_path) in enumerate(zip(frames, frame_paths)):\n",
    "        axes.ravel()[idx].title.set_text(frame_data['frame_type'])\n",
    "        axes.ravel()[idx].imshow(mpimg.imread(frame_path, format='jpeg'))\n",
    "    plt.subplots_adjust(wspace=.05, hspace=.05)\n",
    "    plt.show()\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dced75d",
   "metadata": {},
   "source": [
    "***\n",
    "\n",
    "# Forecasting Hands and Objects (FHO)\n",
    "A **video** has one or more **intervals** which contain **actions** made up of **frames** which define **bounding boxes**.\n",
    "\n",
    "[Data Overview](https://ego4d-data.org/docs/benchmarks/hands-and-objects/)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "134f2590",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This takes long.\n",
    "with pathmgr.open(os.path.join(CLI_OUTPUT_DIR, VERSION, 'annotations', 'fho_main.json'), \"r\") as f:\n",
    "    fho_annotations = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46b1f2e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "fho_video_uids = [e[\"video_uid\"] for e in fho_annotations[\"videos\"]]\n",
    "def get_annotation(video_uid='353ae622-c322-443e-95b4-e9927dedfa1c'):\n",
    "    video_annotation_index = fho_video_uids.index(video_uid)\n",
    "    return fho_annotations[\"videos\"][video_annotation_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdf426a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "debug_show_annotation = False\n",
    "if debug_show_annotation:\n",
    "    ann = get_annotation(video_uid=\"d38f271c-53f3-4771-8c17-a5157e723067\")\n",
    "    print(ann[\"video_metadata\"])\n",
    "    print(ann[\"annotated_intervals\"][0].keys())\n",
    "\n",
    "debug_show_narration = False\n",
    "if debug_show_narration:\n",
    "    video_i = 1\n",
    "    inter_i = 0\n",
    "    action_i = 0\n",
    "    narr_a = fho_annotations[\"videos\"][video_i][\"annotated_intervals\"][inter_i][\"narrated_actions\"][action_i]\n",
    "    narr_a.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa12def0",
   "metadata": {},
   "source": [
    "The following section filters videos based on the narration text, however some videos, e.g. (below) are annotated only with fho_scod:object_state_changes.\n",
    "\n",
    "`d38f271c-53f3-4771-8c17-a5157e723067`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4392802",
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_valid_action(action) -> bool:\n",
    "    if action[\"is_invalid_annotation\"] or action[\"is_rejected\"] or action['stage'] is None:\n",
    "        return False\n",
    "    return True\n",
    "\n",
    "def filter_actions(video, text_q=\"door \", action_q=\"open\"):\n",
    "    for interval in video[\"annotated_intervals\"]:\n",
    "        for narr_action in interval[\"narrated_actions\"]:\n",
    "            if not is_valid_action(narr_action):\n",
    "                continue\n",
    "            if text_q in narr_action[\"narration_text\"] and (action_q is None or narr_action[\"structured_verb\"] == action_q):\n",
    "                yield narr_action\n",
    "\n",
    "debug_filter_actions = False\n",
    "if debug_filter_actions:\n",
    "    text_q, action_q = \"door\", \"open\"\n",
    "    text_q, action_q = \"hammer\", \"take_(pick,_grab,_get)\"\n",
    "    \n",
    "    all_sa = []\n",
    "    all_nt = []\n",
    "    count = 0\n",
    "    for video in fho_annotations[\"videos\"]:\n",
    "        actions = list(filter_actions(video, text_q, action_q))\n",
    "        count += len(actions)\n",
    "        for action in actions:\n",
    "            all_sa.append(action[\"structured_verb\"])\n",
    "            all_nt.append(action[\"narration_text\"])\n",
    "            print(video[\"video_uid\"], action[\"narration_text\"], \"(\", len(actions),\"actions )\")\n",
    "    print(\"done.\", count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d22fe78a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from copy import deepcopy\n",
    "METADATA_PATH = os.path.join(CLI_OUTPUT_DIR, \"ego4d.json\")\n",
    "\n",
    "def load_json_from_path(path):\n",
    "    with open(path) as json_file:\n",
    "        return json.load(json_file)\n",
    " \n",
    "meta = load_json_from_path(METADATA_PATH)\n",
    "\n",
    "metadata = {\n",
    "    video['video_uid']: {\n",
    "        **{\n",
    "            k: v\n",
    "            for k, v in video.items()\n",
    "                if k != 'video_metadata'\n",
    "        },\n",
    "        **video['video_metadata']\n",
    "    }\n",
    "    for video in meta['videos']\n",
    "}\n",
    "def scale_ratio(video_uid, new_height):\n",
    "    return float(new_height) / metadata[video_uid]['display_resolution_height']\n",
    "\n",
    "# scale bboxes - works for the schemas of av, fho_scod, and vq\n",
    "def scale_bboxes(bboxes, sr):\n",
    "    bboxes_new = deepcopy(bboxes)\n",
    "    for i in range(len(bboxes_new)):\n",
    "        bboxes_new[i]['bbox']['x'] = bboxes[i]['bbox']['x'] * sr\n",
    "        bboxes_new[i]['bbox']['y'] = bboxes[i]['bbox']['y'] * sr\n",
    "        bboxes_new[i]['bbox']['width'] = bboxes[i]['bbox']['width'] * sr\n",
    "        bboxes_new[i]['bbox']['height'] = bboxes[i]['bbox']['height'] * sr\n",
    "    return bboxes_new\n",
    "\n",
    "def filter_frames_type(frames, frame_types):\n",
    "    # frame_types must be in the correct order\n",
    "    if frames is None:\n",
    "        return []\n",
    "    frames = list(filter(lambda x: x['frame_type'] in frame_types, frames))\n",
    "    frames = sorted(frames, key=lambda x: x is not None and frame_types.index(x['frame_type']))\n",
    "    return frames\n",
    "\n",
    "def filter_frames(frames, required_objects):\n",
    "    frames_new = []\n",
    "    for i, frame in enumerate(frames):\n",
    "        seen_objects = [x['object_type'] for x in frame['boxes']]\n",
    "        if not np.all([x in seen_objects for x in required_objects]):\n",
    "            continue\n",
    "        frames_new.append(frame)\n",
    "    return frames_new"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90e85147",
   "metadata": {},
   "source": [
    "### Plot a specific video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e47aad4",
   "metadata": {},
   "outputs": [],
   "source": [
    "debug_show_video = True\n",
    "if debug_show_video:\n",
    "    fho_video_uid = \"cde41c4f-50d1-4910-9f2a-4c7b6987df92\" # normal door\n",
    "    #fho_video_uid = \"5d8ebbea-6371-4be5-b611-751fb49ec294\"  # car door\n",
    "    #fho_video_uid = \"40eeac41-9ec3-4960-b0b3-77074a6ad5b3\" # normal door\n",
    "    #fho_video_uid = \"8c335aa9-5caf-4e54-bffb-031ffd452888\"\n",
    "    #fho_video_uid = \"aeadb544-fd9f-4bb8-bfbd-fbefd899ec8e\"\n",
    "    text_q=\"door\"; action_q=\"open\"\n",
    "    \n",
    "    # If this errors out, make sure you used the cli to download the video with this uid\n",
    "    fho_video_path = os.path.join(CLI_OUTPUT_DIR, VERSION, 'video_540ss', fho_video_uid + '.mp4')\n",
    "    print(fho_video_path)\n",
    "    assert os.path.exists(fho_video_path), f\"Video {fho_video_uid} not found. Download it with the cli using: python3 -m ego4d.cli.cli --output_directory=\\\"<output_dir>\\\" --datasets full_scale --video_uids={fho_video_uid} --yes\"\n",
    "    \n",
    "    # Display critical frames for an action as a grid\n",
    "    frame_order = ['pre_45', 'pre_30', 'pre_15', 'pre_frame', 'contact_frame', 'pnr_frame', 'post_frame']    \n",
    "    actions = list(filter_actions(get_annotation(fho_video_uid), text_q, action_q))\n",
    "    \n",
    "    #action = random.sample(actions, 1)[0]\n",
    "    action = actions[0]\n",
    "    \n",
    "    frames = filter_frames_type(action[\"frames\"], frame_order)\n",
    "    print(\"Number of actions:\", len(actions), \"chose:\", actions.index(action), \"text:\", action[\"narration_text\"])\n",
    "    \n",
    "    sr = scale_ratio(fho_video_uid, 540)\n",
    "    frames2 = deepcopy(frames)\n",
    "    for fr in frames2:\n",
    "        fr['boxes'] = scale_bboxes(fr['boxes'], sr)\n",
    "        \n",
    "    plot_frames_with_bboxes(fho_video_path, frames2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3c0ea1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "from pathlib import Path\n",
    "from copy import deepcopy\n",
    "from tqdm import tqdm\n",
    "from PIL import Image\n",
    "from IPython.display import Image as NBImage\n",
    "from nb_video_utils2 import frames2hand, get_frame_centers\n",
    "\n",
    "def fix_object_centers(centers):\n",
    "    fixed_c = []\n",
    "    for i in range(len(centers['left_hand'])):\n",
    "        fixed_point = centers['object_of_change'][i]\n",
    "        fix_objects = fixed_point - centers['object_of_change'] \n",
    "        fixed_objects = centers['object_of_change'] + fix_objects\n",
    "        right_hand2fixed_obj = centers['right_hand'] + fix_objects\n",
    "        left_hand2fixed_obj = centers['left_hand'] + fix_objects\n",
    "        fixed_c.append(dict(object=fixed_objects, left_hand=left_hand2fixed_obj, right_hand=right_hand2fixed_obj,\n",
    "                            name=centers['names'][i]))\n",
    "    return fixed_c\n",
    "\n",
    "def plot_trajectory(idx, centers, frames_dict, hand_choice, act_narration):\n",
    "    i = idx\n",
    "    fixed_c = fix_object_centers(centers)\n",
    "    colors = dict(left_hand='red', right_hand='green', object_of_change='yellow')\n",
    "    fix, ax = plt.subplots(1)\n",
    "    frame_array = frames_dict[fixed_c[i]['name']]\n",
    "    assert isinstance(frame_array, np.ndarray)\n",
    "    #ax.imshow(frame_array)\n",
    "    ax.imshow(np.zeros(frame_array.shape))\n",
    "    ax.set_title(f\"{centers['names'][i]}:{hand_choice[0]}: {act_narration}\")\n",
    "    ax.plot(fixed_c[i]['object'][i:, 0], fixed_c[i]['object'][i:, 1], 'x--', color=colors['object_of_change'], label='OoC')\n",
    "    for hand in ('left_hand', 'right_hand'):\n",
    "        label = hand + \"*\" if hand == hand_choice else \"\"\n",
    "        ax.plot(fixed_c[i][hand][i:, 0], fixed_c[i][hand][i:, 1], 'x--', color=colors[hand], label=label)\n",
    "    ax.legend()\n",
    "    plt.show()\n",
    "\n",
    "def centers2kp_tensor(centers):\n",
    "    # kp_tensor: [CH x no_frames*no_persons*no_keypoints]\n",
    "    # CH: x, y, confidence, mask, frame-number, person-id, keypoint-id\n",
    "    keypoint_names = ('left_hand', 'right_hand', 'object_of_change')\n",
    "    no_CH = 7\n",
    "    no_frames = len(centers['left_hand'])\n",
    "    no_persons = 1\n",
    "    no_keypoints = len(keypoint_names)\n",
    "    kp_tensor = np.zeros((no_CH, no_frames, no_persons, no_keypoints))\n",
    "    for kp_index, kp_name in enumerate(keypoint_names):\n",
    "        for frame_index in range(no_frames):\n",
    "            xy = centers[kp_name][frame_index]\n",
    "            confidence = 0\n",
    "            mask = 1 if not np.any(np.isnan(xy)) else 0\n",
    "            person_id = 0\n",
    "            keypoint_id = kp_index\n",
    "            entry = [*xy, confidence, mask, frame_index, person_id, keypoint_id]\n",
    "            kp_tensor[:, frame_index, 0, kp_index] = entry\n",
    "    return kp_tensor\n",
    "    \n",
    "def loop_videos_and_actions(text_q, action_q, frame_types):\n",
    "    for video in tqdm(fho_annotations[\"videos\"]):\n",
    "        fho_video_path = Path(CLI_OUTPUT_DIR) / VERSION / 'video_540ss' / (video[\"video_uid\"] + '.mp4')\n",
    "        if not fho_video_path.is_file():\n",
    "            continue\n",
    "        actions = list(filter_actions(video, text_q, action_q))\n",
    "        if len(actions) == 0:\n",
    "            continue\n",
    "        sr = scale_ratio(video[\"video_uid\"], 540)\n",
    "        \n",
    "        for a, action in enumerate(actions):\n",
    "            frames = filter_frames_type(action[\"frames\"], frame_types)\n",
    "            if len(frames) == 0:\n",
    "                continue\n",
    "            hand_choice = frames2hand(frames)\n",
    "            frames_f = filter_frames(frames, required_objects=('object_of_change', hand_choice))\n",
    "            if len(frames_f) < 2:\n",
    "                continue\n",
    "                \n",
    "            frames2 = deepcopy(frames_f)\n",
    "            for fr in frames2:\n",
    "                fr['boxes'] = scale_bboxes(fr['boxes'], sr)\n",
    "                \n",
    "            yield fho_video_path, frames2, hand_choice, action['narration_text']\n",
    "\n",
    "plot_dir = Path(\"/tmp/ego4d_plot/\")\n",
    "plot_dir.mkdir(parents=True, exist_ok=True)\n",
    "frame_types = ['pre_45', 'pre_30', 'pre_15', 'pre_frame', 'contact_frame', 'pnr_frame', 'post_frame']\n",
    "\n",
    "#text_q, action_q = \"door\", \"open\"\n",
    "#text_q, action_q = \"bottle\", \"open\"\n",
    "#text_q, action_q = \"sponge\", \"wipe\"\n",
    "#text_q, action_q = \"hose\", \"take_(pick,_grab,_get)\"\n",
    "#text_q, action_q = \"\", \"cut_(trim,_slice,_chop)\"\n",
    "#text_q, action_q = \"cup\", \"take_(pick,_grab,_get)\"\n",
    "text_q, action_q = \"\", \"take_(pick,_grab,_get)\"\n",
    "\n",
    "i = 0\n",
    "for fho_video_path, frames2, hand_choice, act_narration in loop_videos_and_actions(text_q, action_q, frame_types):\n",
    "    # render frames\n",
    "    frame_arrays = get_frames_from_video(fho_video_path, frames2)\n",
    "    frames_dict = dict([(fr['frame_type'],fn) for fr, fn in zip(frames2, frame_arrays)])\n",
    "    #plot_frames(frame_paths, frames)\n",
    "\n",
    "    # get keypoints\n",
    "    centers = get_frame_centers(frames2)\n",
    "    kp_tensor = centers2kp_tensor(centers)\n",
    "    idx = 0\n",
    "    #plot_trajectory(idx, centers, frames_dict, hand_choice, act_narration)\n",
    "    \n",
    "    #print(kp_tensor.shape, hand_choice, act_narration)\n",
    "    \n",
    "    max_images = 10\n",
    "    if i > max_images:\n",
    "        print(f\"Stopping after image limit: {max_images}\")\n",
    "        break\n",
    "\n",
    "print(\"done. number of samples\", i)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "065ba68b-2c8f-4b7c-a044-029e36aafa78",
   "metadata": {},
   "source": [
    "# Keypoint-Diffusion Notes\n",
    "\n",
    "Potential Data Improvements:\n",
    "1. Last datapoint is from post_frame, should probably be removed as its quite random.\n",
    "2. Left-Hand and Right-Hand keypoints are given, hand_choice specifies which is important.\n",
    "3. (maybe) filter trajectories by pixel-distance heuristic / number of movements that go closer.\n",
    "4. Improve relative trajectory estimation through grasp point tracking.\n",
    "5. Estimate hand poses (especially for grasping frame)\n",
    "\n",
    "Potential Model Improvements:\n",
    "1. Default Case: condition on object-keypoint\n",
    "2. Condition on Image\n",
    "3. Condition on Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2937ec43-ffe7-4a6d-999e-a76f06e1f5e2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ef5f1222-4144-4c1d-903a-70d851efe40f",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "708bcb08-b930-48e8-8727-aba034ed651b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(frames_dict.keys())\n",
    "plt.imshow(frames_dict[\"pre_15\"])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2f0479b",
   "metadata": {},
   "source": [
    "# Debugging Stuff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "830ea8b9-f47e-4270-b7bc-ae5d011da22c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_frames(frame_paths, frames):\n",
    "    for frame_path, frame_data in zip(frame_paths, frames):\n",
    "        dest = plot_dir / f'{video[\"video_uid\"]}_{frame_data[\"frame_number\"]:08d}.jpg'\n",
    "        shutil.copyfile(frame_path, dest)\n",
    "        #display(NBImage(filename=frame_path))\n",
    "        #print(f\"â†‘ a:{a} {frame_data['frame_type']} frame num: {frame_data['frame_number']}  -> {dest}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c4add1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_nt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "685e5173",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "sa_list = []\n",
    "for i in range(len(all_nt)):\n",
    "    if \"door\" in all_nt[i]:\n",
    "        print(all_nt[i],\"|\", all_sa[i])\n",
    "        sa_list.append(all_sa[i])\n",
    "Counter(sa_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ecc65cb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3ee6a1c2",
   "metadata": {},
   "source": [
    "# Original Notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0256bc3b-cab1-4bd3-a264-c4d08d3bfc95",
   "metadata": {},
   "outputs": [],
   "source": [
    "# in: video_path, frames: [{ frame_number, frame_label, ?boxes: [{ label, bbox: {x, y, width, height }}] }]\n",
    "# out: matplotlib.ArtistAnimation of frames rendered with bounding boxes (if provided)\n",
    "def render_frames_animation(video_path, frames, **kwargs):\n",
    "    fig, ax = plt.subplots(figsize=(15, 9))\n",
    "    camera = Camera(fig)\n",
    "    for frame in frames:\n",
    "        boxes = frame.get('boxes', [])\n",
    "        frame_path = render_frame_with_bboxes(video_path, frame['frame_number'], boxes)\n",
    "        ax.text(0, 1.01, frame['frame_label'], fontsize=20.0, transform=ax.transAxes)\n",
    "        plt.imshow(mpimg.imread(frame_path, format='jpeg'))\n",
    "        camera.snap()\n",
    "    plt.close(fig)\n",
    "    return camera.animate(**kwargs)\n",
    "\n",
    "# in: segments: [{<start_key>: int, <end_key>: int}]\n",
    "# out: void; as a side effect, renders a plot showing all segments passed in\n",
    "def plot_segments(segments, start_key, end_key):\n",
    "    ordered_segs = sorted(segments, key=lambda x: x[start_key])\n",
    "    lines = [[(x[start_key], i), (x[end_key], i)] for i, x in enumerate(ordered_segs)]\n",
    "\n",
    "    lc = mc.LineCollection(lines, linewidths=2)\n",
    "    fig, ax = plt.subplots(figsize=(30, 10))\n",
    "    ax.add_collection(lc)\n",
    "    ax.autoscale()\n",
    "    ax.set_xlabel('Frame', fontsize=15)\n",
    "    ax.set_ylabel('Segment', fontsize=15)\n",
    "    start, end = ax.get_xlim()\n",
    "    stepsize = (end-start)/30\n",
    "    ax.xaxis.set_ticks(np.arange(start, end, stepsize))\n",
    "    plt.show()\n",
    "    \n",
    "# in: track: [ [{<start_key>: int, <end_key>: int, <label>: str}] ]\n",
    "# out: void; as a side effect, renders a plot showing segments of each track passed in\n",
    "def plot_multitrack_segments(tracks, start_key, end_key, label_key):\n",
    "    cmap = plt.cm.get_cmap('tab20')\n",
    "    color_palette = [cmap(x) for x in range(0, 20)]\n",
    "    \n",
    "    lines, colors, patches = [], [], []\n",
    "    for i, segments in enumerate(tracks):\n",
    "        lines += [[(x[start_key], i), (x[end_key], i)] for x in segments]\n",
    "        color = color_palette[i % len(color_palette)]\n",
    "        colors += [color for _ in segments]\n",
    "        patches += [mpatches.Patch(color=color, label=segments[0][label_key])]\n",
    "\n",
    "    lc = mc.LineCollection(lines, colors = colors, linewidths=550/len(tracks))\n",
    "    fig, ax = plt.subplots(figsize=(30, 10))\n",
    "    ax.legend(handles=patches, loc='upper left')\n",
    "    ax.add_collection(lc)\n",
    "    ax.autoscale()\n",
    "    ax.set_xlabel('Frame', fontsize=15)\n",
    "    ax.set_ylabel('Track', fontsize=15)\n",
    "    start, end = ax.get_xlim()\n",
    "    stepsize = (end-start)/30\n",
    "    ax.xaxis.set_ticks(np.arange(start, end, stepsize))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57b11a5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "fho_ann_video_uids = [e[\"video_uid\"] for e in fho_annotations[\"videos\"][:]]\n",
    "print(f\"FHO: {len(fho_ann_video_uids)} videos - top level: {fho_annotations.keys()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1779d187",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load FHO\n",
    "with pathmgr.open(os.path.join(CLI_OUTPUT_DIR, VERSION, 'annotations', 'fho_main.json'), \"r\") as f:\n",
    "    fho_annotations = json.load(f)\n",
    "    fho_ann_video_uids = list(fho_annotations['video_data'].keys())\n",
    "print(f\"FHO: {len(fho_ann_video_uids)} videos - top level: {fho_annotations.keys()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8111a85c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample one video\n",
    "fho_video_uid = random.sample(fho_ann_video_uids, 1)[0]\n",
    "fho_video = videos_df[videos_df.video_uid == fho_video_uid].iloc[0]\n",
    "print(f\"Sampled Video: {fho_video}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faa71e59",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summarize FHO Annotations\n",
    "fho_video_annotations = fho_annotations['video_data'].get(fho_video_uid)\n",
    "annotation_intervals = fho_video_annotations['annotated_intervals']\n",
    "print(f\"Video Intervals: {len(annotation_intervals)}, uid: {fho_video_uid}\")\n",
    "for interval in annotation_intervals:\n",
    "    print(f\"Interval [{interval['start_sec']} - {interval['end_sec']}]\")\n",
    "    actions = list(filter(lambda x: not (x['is_invalid_annotation'] or x['is_rejected']) and x['stage'] is not None, interval['narrated_actions']))\n",
    "    print(f\"Actions: {len(actions)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5e4e0a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# If this errors out, make sure you used the cli to download the video with this uid\n",
    "fho_video_path = os.path.join(CLI_OUTPUT_DIR, VERSION, 'full_scale', fho_video_uid + '.mp4')\n",
    "assert os.path.exists(fho_video_path), f\"Video {fho_video_uid} not found. Download it with the cli using: python3 -m ego4d.cli.cli --output_directory=\\\"<output_dir>\\\" --datasets full_scale --video_uids={fho_video_uid} --yes\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d35007d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display critical frames for an action as a grid\n",
    "frame_order = ['pre_45', 'pre_30', 'pre_15', 'pre_frame', 'contact_frame', 'pnr_frame', 'post_frame']\n",
    "\n",
    "interval = random.sample(fho_video_annotations['annotated_intervals'], 1)[0]\n",
    "actions = list(filter(lambda x: not (x['is_invalid_annotation'] or x['is_rejected']) and x['stage'] is not None, interval['narrated_actions']))\n",
    "action = random.sample(actions, 1)[0]\n",
    "frames = sorted(action['frames'], key=lambda x: frame_order.index(x['frame_type']))\n",
    "plot_frames_with_bboxes(fho_video_path, frames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e44af5ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display critical frames as an animation\n",
    "labeled_frames = list(map(\n",
    "    lambda frame:\n",
    "        {\n",
    "            'frame_label': f\"{frame['frame_type']}:{frame['frame_number']}\",\n",
    "            **frame\n",
    "        }, frames))\n",
    "render_frames_animation(fho_video_path, labeled_frames, interval=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "487e7c33",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select a single frame\n",
    "sample_frame_priority = ['contact_frame', 'pnr_frame', 'pre_frame', 'post_frame', 'pre_15', 'pre_30', 'pre_45']\n",
    "sample_frame = sorted(action['frames'], key=lambda x: sample_frame_priority.index(x['frame_type']))[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83af9cdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display bounding boxes on a single frame\n",
    "sample_frame_path = render_frame_with_bboxes(fho_video_path, sample_frame['frame_number'], sample_frame['boxes'])\n",
    "plt.rcParams['figure.figsize'] = [20, 20]\n",
    "plt.title(sample_frame['frame_type'])\n",
    "plt.axis('off')\n",
    "plt.imshow(mpimg.imread(sample_frame_path, format='jpeg'))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9f0b0fa",
   "metadata": {},
   "source": [
    "***\n",
    "\n",
    "# Visual Object Queries (VQ)\n",
    "[Data Overview](https://ego4d-data.org/docs/data-overview/#visual-object-queries)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8478d83b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load VQ\n",
    "with pathmgr.open(os.path.join(CLI_OUTPUT_DIR, VERSION, 'annotations', 'vq_train.json'), \"r\") as f:\n",
    "    vq_annotations = json.load(f)\n",
    "    vq_ann_video_uids = [x[\"video_uid\"] for x in vq_annotations[\"videos\"]]\n",
    "vq_video_dict = {x[\"video_uid\"]: x[\"clips\"] for x in vq_annotations[\"videos\"]}\n",
    "print(f\"VQ: {len(vq_ann_video_uids)} videos - top level: {vq_annotations.keys()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3abb7a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample one video\n",
    "vq_video_uid = random.sample(vq_ann_video_uids, 1)[0]\n",
    "vq_video = videos_df[videos_df.video_uid == vq_video_uid].iloc[0]\n",
    "print(f\"Sampled Video: {vq_video}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a709bee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summarize VQ Annotations\n",
    "vq_video_annotations = vq_video_dict.get(vq_video_uid)\n",
    "print(f\"VQ Video: {len(vq_video_annotations)}, clips: {vq_video_uid}\")\n",
    "print(f\"clip keys: {vq_video_annotations[0].keys()}\")\n",
    "print(f\"query set keys: {vq_video_annotations[0]['annotations'][0]['query_sets']['1'].keys()}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bcebb20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# If this errors out, make sure you used the cli to download the video with this uid\n",
    "vq_video_path = os.path.join(CLI_OUTPUT_DIR, VERSION, 'full_scale', vq_video_uid + '.mp4')\n",
    "assert os.path.exists(vq_video_path), f\"Video {vq_video_uid} not found. Download it with the cli using: python3 -m ego4d.cli.cli --output_directory=\\\"<output_dir>\\\" --datasets full_scale --video_uids={vq_video_uid} --yes\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3879e79",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display visual crop frame\n",
    "query_set = vq_video_annotations[0]['annotations'][0]['query_sets']['1']\n",
    "object_frame = query_set['visual_crop']\n",
    "box = {\n",
    "    'object_type': query_set['object_title'],\n",
    "    'bbox': {\n",
    "        'x': object_frame['x'],\n",
    "        'y': object_frame['y'],\n",
    "        'width': object_frame['width'],\n",
    "        'height': object_frame['height'],\n",
    "    }\n",
    "}\n",
    "object_frame_path = render_frame_with_bboxes(vq_video_path, object_frame['video_frame_number'], [box])\n",
    "plt.rcParams['figure.figsize'] = [20, 20]\n",
    "plt.imshow(mpimg.imread(object_frame_path, format='jpeg'))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff3d42f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display animated response track\n",
    "response_track_frames = query_set['response_track']\n",
    "reformatted_frames = list(map(\n",
    "    lambda frame:\n",
    "        {\n",
    "            'frame_number': frame['video_frame_number'],\n",
    "            'frame_label': f\"Frame: {frame['video_frame_number']}\",\n",
    "            'boxes': [{\n",
    "                'object_type': query_set['object_title'],\n",
    "                'bbox': {\n",
    "                    'x': frame['x'],\n",
    "                    'y': frame['y'],\n",
    "                    'width': frame['width'],\n",
    "                    'height': frame['height'],\n",
    "                }\n",
    "            }]\n",
    "        }, response_track_frames))\n",
    "render_frames_animation(vq_video_path, reformatted_frames, interval=200)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95f590ab",
   "metadata": {},
   "source": [
    "# Natural Language Queries (NLQ)\n",
    "\n",
    "[Data Overview](https://ego4d-data.org/docs/data-overview/#natural-language-queries)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "495343ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load NLQ\n",
    "with pathmgr.open(os.path.join(CLI_OUTPUT_DIR, VERSION, 'annotations', 'nlq_train.json'), \"r\") as f:\n",
    "    nlq_annotations = json.load(f)\n",
    "    nlq_ann_video_uids = [x[\"video_uid\"] for x in nlq_annotations[\"videos\"]]\n",
    "nlq_video_dict = {x[\"video_uid\"]: x[\"clips\"] for x in nlq_annotations[\"videos\"]}\n",
    "print(f\"NLQ: {len(nlq_ann_video_uids)} videos - top level: {nlq_annotations.keys()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "769023a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample one video\n",
    "nlq_video_uid = random.sample(nlq_ann_video_uids, 1)[0]\n",
    "nlq_video = videos_df[videos_df.video_uid == nlq_video_uid].iloc[0]\n",
    "print(f\"Sampled Video: {nlq_video}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "104ff2ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summarize NLQ Annotations\n",
    "nlq_video_annotations = nlq_video_dict.get(nlq_video_uid)\n",
    "print(f\"NLQ Video: {len(nlq_video_annotations)}, clips: {nlq_video_uid}\")\n",
    "print(f\"clip keys: {nlq_video_annotations[0].keys()}\")\n",
    "print(f\"language_query keys: {nlq_video_annotations[0]['annotations'][0]['language_queries'][0].keys()}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d17e680c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# If this errors out, make sure you used the cli to download the video with this uid\n",
    "nlq_video_path = os.path.join(CLI_OUTPUT_DIR, VERSION, 'full_scale', nlq_video_uid + '.mp4')\n",
    "assert os.path.exists(nlq_video_path), f\"Video {nlq_video_uid} not found. Download it with the cli using: python3 -m ego4d.cli.cli --output_directory=\\\"<output_dir>\\\" --datasets full_scale --video_uids={nlq_video_uid} --yes\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bbb55e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the clip for a language query\n",
    "nlq_clip = random.sample(nlq_video_annotations, 1)[0]\n",
    "nlq_clip_annotations = random.sample(nlq_clip['annotations'], 1)[0]\n",
    "sample_nlq_language_query = random.sample(nlq_clip_annotations['language_queries'], 1)[0]\n",
    "\n",
    "[print(k.capitalize(),':',v) for k, v in sample_nlq_language_query.items()]\n",
    "\n",
    "# only render 15 frames for performance\n",
    "nlq_stepsize = (sample_nlq_language_query[\"video_end_frame\"] - sample_nlq_language_query[\"video_start_frame\"]) // 15 \n",
    "render_frames_animation(\n",
    "    nlq_video_path,\n",
    "    [\n",
    "        {\"frame_number\": x, \"frame_label\": f\"Frame {x}\"}\n",
    "        for x in range(\n",
    "            sample_nlq_language_query[\"video_start_frame\"],\n",
    "            sample_nlq_language_query[\"video_end_frame\"],\n",
    "            nlq_stepsize\n",
    "        )\n",
    "    ],\n",
    "    interval = 400\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05b5fa18",
   "metadata": {},
   "source": [
    "***\n",
    "\n",
    "# Audio-Visual Diarization (AV)\n",
    "[Data Overview](https://ego4d-data.org/docs/benchmarks/AV-diarization/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ee32ef4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load AV\n",
    "with pathmgr.open(os.path.join(CLI_OUTPUT_DIR, VERSION, 'annotations', 'av_train.json'), \"r\") as f:\n",
    "    av_annotations = json.load(f)\n",
    "    av_ann_video_uids = [x[\"video_uid\"] for x in av_annotations[\"videos\"]]\n",
    "av_video_dict = {x[\"video_uid\"]: x[\"clips\"] for x in av_annotations[\"videos\"]}\n",
    "print(f\"AV: {len(av_ann_video_uids)} videos - top level: {av_annotations.keys()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26ff5f0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample one video\n",
    "av_video_uid = random.sample(av_ann_video_uids, 1)[0]\n",
    "av_video = videos_df[videos_df.video_uid == av_video_uid].iloc[0]\n",
    "print(f\"Sampled Video: {av_video}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e189b23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summarize AV Annotations\n",
    "av_video_annotations = av_video_dict.get(av_video_uid)\n",
    "print(f\"AV Video: {len(av_video_annotations)}, clips: {av_video_uid}\")\n",
    "print(f\"clip keys: {av_video_annotations[0].keys()}\")\n",
    "print(f\"tracked people: {len(av_video_annotations[0]['persons'])}\")\n",
    "print(f\"person keys: {av_video_annotations[0]['persons'][0].keys()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85b9258a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aggregate av tracking bounding boxes in an indexable dictionary\n",
    "def get_av_frame_dict(av_video_annotation):\n",
    "    frame_aggregator = {}\n",
    "    for person in av_video_annotation['persons']:\n",
    "        for tracking_path in person['tracking_paths']:\n",
    "            for track in tracking_path['track']:\n",
    "                frame = frame_aggregator.get(track['video_frame'])\n",
    "                if frame is None:\n",
    "                    frame = {\n",
    "                        \"frame_number\": track['video_frame'],\n",
    "                        \"frame_label\": f\"Frame: {track['video_frame']}\",\n",
    "                        \"frame_type\": f\"Frame: {track['video_frame']}\",\n",
    "                        \"boxes\": []\n",
    "                    }\n",
    "                frame['boxes'].append({\n",
    "                    \"object_type\": tracking_path['track_id'],\n",
    "                    \"bbox\": {\n",
    "                        \"x\": track['x'],\n",
    "                        \"y\": track['y'],\n",
    "                        \"width\": track['width'],\n",
    "                        \"height\": track['height']\n",
    "                    }\n",
    "                })\n",
    "                frame_aggregator[track['video_frame']] = frame\n",
    "    return frame_aggregator\n",
    "\n",
    "    \n",
    "# Get ordered list of frames \n",
    "def get_av_frames_with_bboxes(av_video_annotation):\n",
    "    frame_dict = get_av_frame_dict(av_video_annotation)\n",
    "    return sorted(list(frame_dict.values()), key=lambda x: x['frame_number'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "793858fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# If this errors out, make sure you used the cli to download the video with this uid\n",
    "av_video_path = os.path.join(CLI_OUTPUT_DIR, VERSION, 'full_scale', av_video_uid + '.mp4')\n",
    "assert os.path.exists(av_video_path), f\"Video {av_video_uid} not found. Download it with the cli using: python3 -m ego4d.cli.cli --output_directory=\\\"<output_dir>\\\" --datasets full_scale --video_uids={av_video_uid} --yes\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76a974b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aggregate frames from av person tracking\n",
    "av_tracked_frame_dict = get_av_frame_dict(av_video_annotations[0])\n",
    "av_tracked_frames = get_av_frames_with_bboxes(av_video_annotations[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a486c75",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot a random sample of tracked frames\n",
    "plot_frames_with_bboxes(av_video_path, random.sample(av_tracked_frames, 9))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65c71286",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show an animation of a subset of frames\n",
    "render_frames_animation(av_video_path, av_tracked_frames[:25], interval=60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43ec1272",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot a single random tracked av frame\n",
    "sample_frame = random.sample(av_tracked_frames, 1)[0]\n",
    "\n",
    "frame_path = render_frame_with_bboxes(av_video_path, sample_frame['frame_number'], sample_frame['boxes'])\n",
    "plt.rcParams['figure.figsize'] = [20, 20]\n",
    "plt.imshow(mpimg.imread(frame_path, format='jpeg'))\n",
    "plt.title(f\"Frame: {sample_frame['frame_number']}\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73da6879",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize all social segments in the video\n",
    "plot_segments(av_video_annotations[0]['social_segments'], 'video_start_frame', 'video_end_frame')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b92e211c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize all peoples' voice segments\n",
    "person_voice_track_segments = [x['voice_segments'] for x in\n",
    "                               av_video_annotations[0]['persons']\n",
    "                               if len(x['voice_segments']) > 0]\n",
    "for track in person_voice_track_segments:\n",
    "    for segment in track:\n",
    "        segment['label'] = f\"Person {segment['person']}\"\n",
    "ordered_tracks = sorted(person_voice_track_segments, key=lambda x: x[0]['video_start_frame'])\n",
    "plot_multitrack_segments(ordered_tracks, 'video_start_frame', 'video_end_frame', 'label')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3417d30f",
   "metadata": {},
   "source": [
    "# Searching for Actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63650586",
   "metadata": {},
   "outputs": [],
   "source": [
    "# show an example of an narrated action\n",
    "#fho_annotations[\"videos\"][11][\"annotated_intervals\"][0][\"narrated_actions\"][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08699515",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_objects(action):\n",
    "    sn = set()\n",
    "    frames = action['frames']\n",
    "    if frames is None:\n",
    "        return []\n",
    "    for frame in action['frames']:\n",
    "        for box in frame['boxes']:\n",
    "            sn.add(box['_structured_noun'])\n",
    "    if None in sn:\n",
    "        sn.remove(None)\n",
    "    return list(sn)\n",
    "\n",
    "\n",
    "all_sa = []\n",
    "all_nt = []\n",
    "all_sn = []\n",
    "for video in fho_annotations[\"videos\"]:\n",
    "    for interval in video[\"annotated_intervals\"]:\n",
    "        for action in interval[\"narrated_actions\"]:\n",
    "            if not is_valid_action(action):\n",
    "                continue\n",
    "            sv = action[\"structured_verb\"]\n",
    "            #if sv != 'take_(pick,_grab,_get)':\n",
    "            #    continue\n",
    "            \n",
    "            all_sa.append(action[\"structured_verb\"])\n",
    "            all_nt.append(action[\"narration_text\"])\n",
    "            sn = get_objects(action)\n",
    "            if len(sn) == 0:\n",
    "                sn = [\"\"]\n",
    "            if len(sn) > 1:\n",
    "                sn = sn[:1]\n",
    "            all_sn.extend(sn)\n",
    "            #print(video[\"video_uid\"], action[\"narration_text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2c8082b-17e6-46ab-ba21-59a8d44e9053",
   "metadata": {},
   "outputs": [],
   "source": [
    "Counter(all_sa)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3faf03cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "mask_sa = np.array(all_sa) == 'take_(pick,_grab,_get)'\n",
    "mask_sn = np.array(all_sn) == 'container_(box,_can,_carton,_case,_casing,_container,_crate,_holder,_jar,_jerrycan,_keg,_pack,_package,_packaging,_packet,_storage,_tank,_tin)'\n",
    "mask = np.logical_and(mask_sa,mask_sn)\n",
    "\n",
    "sel_nt = np.array(all_nt)[mask]\n",
    "\n",
    "print(Counter(sel_nt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81d48fba",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "object_names = ('bowl', 'cup', 'glass', 'bottle','box','knife', 'hammer','pen','carrot')\n",
    "#object_names = ('mug', 'bowl', 'bottle', 'box', 'can', 'headphones')\n",
    "\n",
    "for obj_name in object_names:\n",
    "    print(obj_name.ljust(10), len([True for x in all_nt if obj_name in x]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3155bc20",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "with open(\"narration_text_pick.json\", 'w') as f:\n",
    "    json.dump(all_nt, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4663b26",
   "metadata": {},
   "outputs": [],
   "source": [
    "for k,v in metadata.items():\n",
    "    print(v.keys())\n",
    "    break\n",
    "    \n",
    "stereo = []\n",
    "for k,v in metadata.items():\n",
    "    stereo.append(v['imu_metadata'] is not None)\n",
    "    \n",
    "    #print(stereo)\n",
    "    #break\n",
    "#print(stereo)\n",
    "#plt.hist(stereo)\n",
    "#plt.show()\n",
    "np.mean(stereo)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0159601b",
   "metadata": {},
   "source": [
    "# Notes: \n",
    "mono-depth-estimation: https://github.com/EPFL-VILAB/omnidata/tree/main/omnidata_tools/torch#readme\n",
    "\n",
    "STA training code: https://ego4d-data.org/docs/challenge/\n",
    "\n",
    "EgoTracks: object tracking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "846bc455",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
